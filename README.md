# L1-vs-L2-Regularization-in-Machine-Learning-with-Code-Examples
A hands-on tutorial to understand L1 (Lasso) and L2 (Ridge) regularization using Python and Scikit-learn with visual and performance comparison.

This repository provides a detailed and practical demonstration of how L1 (Lasso) and L2 (Ridge) regularization work in various machine learning models. It includes:

- Explanation of L1 vs L2 regularization
- When to use L1 over L2 (and vice versa)
- Separate performance analysis for Linear Regression, Lasso, and Ridge
- Visual comparison of model coefficients
- Examples of regularization in Logistic Regression, SVM, XGBoost, LightGBM, and Neural Networks

This is ideal for beginners and intermediate ML learners who want to develop an intuitive and applied understanding of how regularization can prevent overfitting and improve model generalization.

## ðŸ“˜ Whatâ€™s Inside?

- âœ… Intuitive explanation of L1 vs L2 regularization
- âœ… Code examples using Linear Regression, Lasso, and Ridge
- âœ… Performance metrics: MSE and RÂ² score
- âœ… Visual coefficient comparison
- âœ… Use of regularization in:
  - Logistic Regression
  - Support Vector Machines
  - XGBoost & LightGBM
  - Keras (Neural Networks)

## ðŸ’¡ When to Use What?

- Use **L1 (Lasso)** when:
  - You want feature selection (sparse model)
  - You believe many features are irrelevant

- Use **L2 (Ridge)** when:
  - You want to penalize large coefficients but keep all features
  - Your features are correlated (multicollinearity)
  - 
